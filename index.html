    <!doctype html>
<html lang="en">

<head>
	<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-17772MEV4Q"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-17772MEV4Q');
</script>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <title>Robotics Engineer</title>
  <link rel="icon" type="image/x-icon" href="assets/emoji_1.png">


  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
    <style>
    html {
      scroll-behavior: smooth;
    }

      body {
        margin: 0;
        font-family: 'Merriweather', serif;
        font-size: 15px;
        line-height: 1.6;
        color: #333;
      }

      h1 {
      font-size: 24px;
      font-weight: bold;
      border-bottom: 2px solid #000;
      display: inline-block;
    }
    h2 {
      font-size: 20px;
      font-weight: bold;
      margin-top: 10px;
    }
    .h2_nik {
      font-size: 20px;
      font-weight: bold;
      margin-top: 30px;
    }
    ul {
      margin-top: 0;
      margin-bottom: 10px;
    }
    li {
      margin-bottom: 4px;
    }
    .li_nik {
      margin-bottom: 8px;
      margin-top: 4px;
    }
    .degree {
      font-weight: bold;
    }
    .school {
      font-style: italic;
    }
    .date {
      float: right;
    } 
.header {
  display: flex;
  align-items: center;
  justify-content: center;
  background-color: #eee;
  padding: 1rem;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  z-index: 9999;
}

.header__logo {
  font-weight: bold;
  font-size: 1.5rem;
  margin-right: auto;
}

.header__nav {
  display: flex;
  align-items: center;
}

.header__nav__list {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  justify-content: center;
  margin-left: auto;
}

.header__nav__item {
  margin-left: 1.5rem;
}

.header__nav__item:first-child {
  margin-left: 0;
}

.header__nav__item a {
  text-decoration: none;
  color: #333;
  font-weight: bold;
}

      .gpa {
        font-weight: bold;
        margin-top: 5px;
      }
      .experience {
      background-color: #f8f8f8;
      padding: 80px 0;
      }

      .experience__item {
        margin-bottom: 30px;
      }

        .experience__item__title {
          margin: 0;
          font-size: 24px;
          font-weight: bold;
        }

        .experience__item__meta {
          margin: 10px 0;
          font-size: 14px;
        }

        .experience__item__meta__company {
          margin-right: 10px;
        }

        .experience__item__meta__location {
          margin-right: 10px;
        }

        .experience__item__meta__date {
          margin-right: 10px;
        }

        .experience__item__responsibilities {
          margin: 0;
          padding: 0 0 0 20px;
        }

        .experience__item__responsibilities li {
          margin-bottom: 10px;
          font-size: 16px;
        }

        .competitions {
          margin-top: 40px;
        }

        .competitions__list {
          list-style: none;
          padding: 0;
          margin: 0;
        }

        .competitions__item {
          margin-bottom: 20px;
        }

        .competitions__title {
          margin-bottom: 10px;
          font-size: 24px;
        }

        .competitions__title_award {
          margin-bottom: 10px;
          font-size: 18px;
        }

        .competitions__description {
          margin-bottom: 10px;
          font-size: 15px;
        }

        .competitions__result_red {
          font-style: italic;
          color: #FF0000;
        }
        .end {
          font-size: 15px;
        }
        
        @media (max-width: 767px) {
			.header {
				display: none;
			}

      </style>
</head>

<body>
    <header class="header">
  <div class="header__logo">Hemanth Surya</div>
  <nav class="header__nav">
    <ul class="header__nav__list">
      <li class="header__nav__item"><a href="#about-me" class="scroll-to">About Me</a></li>
      <li class="header__nav__item"><a href="#experience">Experience</a></li>
      <li class="header__nav__item"><a href="#skills">Skills</a></li>
      <li class="header__nav__item"><a href="#projects">Projects</a></li>
      <li class="header__nav__item"><a href="#publications">Publications</a></li>
      <li class="header__nav__item"><a href="#competitions">Competitions</a></li>
      <li class="header__nav__item"><a href="#awards">Awards</a></li>
    </ul>
  </nav>
</header>


    <div class="container">
        <div class="row" style="margin-top: 5em; ">
            <div class="col-sm-12" style="margin-bottom: 1em;" id="about-me">
            <h3 class="display-4" style="text-align: center;"><span style="font-weight: bold;">Hemanth </span>Surya</h3>
            </div>
            <br>
            
            <div class="col-md-8" style="">
                
                <p>
                    <span style="font-weight: bold;">Actively looking for Full-Time Opportunities</span> 
                    
                </p>

                <p>
                    <span style="font-weight: bold;">Bio:</span> 
                    I am currently pursuing my Master's degree in Robotics at <a href="https://www.gatech.edu/" target="_blank">Georgia Institute of Technology</a>, starting from Fall 2022. My current research lies in the field of Object Detection, Localization, tracking and Path generation Algorithms. 
                </p>

                <p>
                    Prior to starting my Master's, I worked as a Cloud Support Associate at <a href="https://aws.amazon.com/?nc2=h_lg" target="_blank">Amazon Web Services</a> in the Autonomous and Automotive business unit. I gained valuable experience in software development for autonomous systems during my time there.
                </p>

                <p>
                    My passion for robotics began during my undergraduate studies at <a href="https://amrita.edu/" target="_blank">Amrita University</a>, where I majored in Electronics and Communications Engineering. I spent three years working as a student researcher at <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/" target="_blank">Humanitarian Technology Labs (HuT Labs) </a>, where I worked on various robotics projects, published research papers, and participated in several competitions.
                </p>

                <p>
                    I am excited to continue my academic journey in the field of robotics and make meaningful contributions to the development of autonomous systems. In the future, I hope to explore new and innovative approaches to perception-based robotics and help shape the future of this rapidly-evolving field.
                </p>
                
                <p>For any collaborations, feel free to reach out to me!</p>
                <p>
                    <a href="https://drive.google.com/file/d/1Tt3slBzpibQRIWZAh-aC0jWG8A_fc48k/view?usp=drive_link" target="_blank" style="margin-right: 15px"><i class="fa fa-address-card fa-lg"></i> Resume</a>
                    <a href="mailto:htammana3@gmail.com, htammana3@gatech.edu" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
                    <a href="https://scholar.google.com/citations?user=es7mlasAAAAJ&hl=en" target="_blank" style="margin-right: 15px"><i class="fa-solid fa-book"></i> Scholar</a>
                    <a href="https://github.com/hsurya08" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
                    <a href="https://www.linkedin.com/in/hsurya/" target="_blank" style="margin-right: 15px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
                </p>
    
            </div>
            <div class="col-md-4" style="">
                <img src="assets/img/hemanth.png" class="img-thumbnail" width="350px" alt="Profile picture">
            </div>
        </div>



        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Professional Experience</h1>
                <h2>Robotics Vision Intern</h2>
                <div class="date">May 2023 - August 2023</div>
                <div class="school">SkyMul robotics</div>
                <ul>
                    <li>Worked on the autonomous navigation of quadruped robots designed for assisting in construction tasks such as layout design, rebar tying, etc. using visual odometry (VIO) and point cloud data obtained from the real-sense cameras</li>
                    <li>Implemented a real-time computer vision algorithm to detect the rebar intersections using the depth camera.</li>                    
                </ul>

                <h2>Cloud Support Associate (IaC)</h2>
                <div class="date">Jan 2022 - August 2022</div>
                <div class="school">Amazon, India 🇮🇳</div>
                <ul>
                    <li>Worked on cloud computing services offered by AWS, such as EC2, S3, CloudFormation, Control Tower, etc. </li>
                    <li>Gained experience in interacting with clients from various fields and backgrounds by troubleshooting and resolving their issues related to these services. Helped them in migrating their infrastructure to AWS. </li>
                </ul>
                
                <h2 class="h2_nik">Graduate Student Researcher</h2>
                <div class="date">Jan 2023 - Present</div>
                <div class="school">LIDAR lab</div>
                <ul>
                    <li>Implemented mapping and navigation techniques such as 2.5D elevation mapping and plane segmentation that can extend the range of exploration for quadrupedal robots using the point cloud data obtained from depth cameras.</li>
                    <li>Implemented Computer Vision algorithms to detect and recognize objects and obstacles in the robot's environment, allowing the robot to navigate safely and efficiently.</li>
                    <li>Designed an algorithm to generate a safe and efficient path for bipedal/quadrupedal robots to follow on different terrains, by detecting and avoiding obstacles. Implemented a navigation system that can switch between different gait controllers.</li>
                </ul>

                <h2 class="h2_nik">Data Analyst</h2>
                <div class="date">June 2021 - Dec 2021</div>
                <div class="school">TheMathCompany</div>
                <ul>
                    <li>Developed a price elasticity simulator to quantify the relation between the price and demand of a company’s product.</li>
                    <li>Provided business insights and performed hypothesis testing to identify various internal and external factors affecting sales.</li>
                </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Skills</h1>

                <div class="row">
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Python</li>
                    <li class="li_nik">C++</li>
                    <li class="li_nik">SQL</li>
                    <li class="li_nik">Deep Learning</li>
                    <li class="li_nik">Machine Learning</li>
                    <li class="li_nik">CUDA</li>
                    <li class="li_nik" id="projects">Embedded Systems</li>
                    <li class="li_nik">Amazon Web Services (AWS)</li>
                  </ul>
                </div>
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Computer Vision</li>
                    <li class="li_nik">SLAM (Simultaneous Localization and Mapping)</li>
                    <li class="li_nik">PyTorch</li>
                    <li class="li_nik">Multi-Threading</li>
                    <li class="li_nik">Matlab</li>
                    <li class="li_nik">ROS (Robot Operating System), ROS2</li>
                    <li class="li_nik">Gazebo, Rviz</li>
                  </ul>
                </div>
              </div>
            </div>
        </div>







        <div class="row" style="margin-top: 3em;">
                    <div class="col-sm-15" style="">
                        <h1>Projects</h1>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/attitude_estimation.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/attitude-estimation-matlab-iphone" target="_blank">Attitude Estimation of iPhone using MATLAB </a><br>This project focuses on the development of a MATLAB-based Attitude Estimation system, designed to accurately determine the orientation of a mobile device using data from its accelerometer and gyroscope sensors. The code implements various estimation techniques, including accelerometer-only estimation, gyroscope-only estimation, a complementary filter, and a Kalman filter. It processes sensor data and provides estimates of roll, pitch, and yaw angles, enabling a comprehensive understanding of the device's orientation in 3D space. <br><a href="https://github.com/GutlapalliNikhil/attitude-estimation-matlab-iphone" target="_blank">Code</a></div> </div> </div>
			    
			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/3d_map_2d_lidar.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/3D-Mapping-Using-2D-LiDAR-ROS" target="_blank">Simulating and Mapping 3D Environments with 2D Lidar in ROS </a><br>In this project, we harnessed the synergy of 2D Lidar, ROS, and Gazebo to pioneer 3D mapping. Our journey began with crafting a dynamic robot in SolidWorks, boasting a Lidar poised to scan from 0 to +60 degrees. A bespoke Lidar plugin orchestrated data collection, translating laser scans into intricate point clouds. Augmented by the octomap library, these clouds unfurled into vivid 3D maps. This venture not only showcases technical finesse but also exemplifies the potential of autonomous spatial understanding.<br><a href="https://github.com/GutlapalliNikhil/3D-Mapping-Using-2D-LiDAR-ROS" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/3D_detection.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Complex YOLO-ROS 3D Object-Detection </a><br>The Complex YOLO ROS 3D Object Detection project is an integration of the Complex YOLOv4 package into the ROS (Robot Operating System) platform, aimed at enhancing real-time perception capabilities for robotics applications. Using 3D object detection techniques based on Lidar data, the project enables robots and autonomous systems to accurately detect and localize objects in a 3D environment, crucial for safe navigation, obstacle avoidance, and intelligent decision-making.<br><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/pointcloud_Classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Point Cloud Classification using PyTorch. </a><br>The Point Cloud Classification project implements the PointNet architecture for classifying 3D point clouds. It supports two datasets, ModelNet and ScanObjectNN, and provides functions for dataset preprocessing, model training, and visualization. The project aims to classify point clouds into different categories using deep learning techniques, enabling applications in object recognition, scene understanding, and robotics. The command-line interface allows easy configuration and training of the model.<br><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/ScorpionImage.jpeg" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://www.youtube.com/watch?v=qSWeLO4bYwI" target="_blank">Autonomous Search and rescue Robot</a><br>In my object detection project, I utilized the pre-trained YOLOv5 model from the Ultralytics repository to detect and classify emotions on human faces. Using the open-source software "labelme," I annotated images to create labeled datasets for training and validation. During the training phase, I fine-tuned the YOLOv5 model, optimizing its parameters to accurately identify "Happy" or "Neutral" expressions. Achieving high accuracy with the trained model, I then deployed the weights on an Android phone, developing a custom application for real-time emotion detection. This project showcases the practical application of object detection techniques for emotion recognition, with the ability to deploy such a solution on mobile devices.<br><a href="https://github.com/NigamKatta/RescueRobot/tree/main" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/image_classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Custom Image Classification using Pretrained Transformer Model (ViT)</a><br>For my custom image classification project, I utilized a pretrained transformer model to achieve accurate results. By fine-tuning the model during the training phase and evaluating its performance during validation, I successfully adapted it to my specific image classification task. The pretrained transformer model's ability to capture intricate features and patterns in images proved highly effective in achieving high classification accuracy.<br><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/collaborative_SLAM.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Collaborative SLAM using Multi-Robot System</a><br>Developed a novel approach to collaborative Simultaneous Localization and Mapping (SLAM) by utilizing a multi-robot system. The experiment employed two turtlebot3 robots operating in a simulated gazebo environment, with ROS serving as the platform for seamless integration. Leveraging the gmapping algorithm, we achieved highly precise mapping of the environment. Additionally, we equipped each robot with autonomous exploration capabilities using explore lite. The maps generated by the robots were skillfully merged using the multirobot map merger, resulting in a comprehensive and detailed representation of the environment.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Project Report</a> / <a href="https://github.com/GutlapalliNikhil/Collaborative_SLAM" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=Mosx_JfMLIQ" target="_blank">Demo Video</a> / <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Collaborative_SLAM.pdf" target="_blank">Slides</a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/sTETRO.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/NigamKatta/sTETRO/tree/main" target="_blank">sTetro: Revolutionizing Staircase-Cleaning Robotics</a><br> I played a key role in making sTetro, a cool robot that's changing how cleaning gets done. I focused on creating its brain - the staircase motion controller. It's smart and uses sensors to make decisions while moving around. I also made sure it can handle different surfaces smoothly by setting up the encoder and IMU-based PID controller. Then, I used fancy tech like Deep CNN and YOLO-v3 to help sTetro spot stairs accurately and climb them by itself. These cool tricks were added into sTetro's software, making it a super-efficient cleaner. Check out my work on GitHub to see how sTetro is shaking up the cleaning game!"<br></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/21.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Collaborative Robotics on Navigational Platform</a><br>This project involves the development of an autonomous wheelchair with an integrated voice and robotic arm. The system aims to perform Human-Robot-Interaction and Cooperation, Navigation and Mapping in dynamic environments, Opening a Door, and Exchanging objects with a Companion. To achieve this, the team developed their own library for navigation and studied the wheelchair dynamics combined with a robotic arm. The system is validated using hardware and can be controlled either through speech commands or a mobile app. The team used ROS, Gmapping, Navigation Stack, and AMCL for navigation, a custom planner named "HuT Planner" for path planning, and a Markov model for speech recognition. They also created an app for speech recognition and utilized kinova software for the robotic arm. The project can find applications in various settings, including airports, households, and shopping malls, to assist physically disabled and elderly individuals in their daily activities.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Thesis</a> / <a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://www.amrita.edu/project/self-e/" target="_blank">More Info </a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/chetak.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><b>CHETAK 🐎 - The Home Service Robot</b><br>CHETAK is a self-governing home assistance robot designed to support individuals with disabilities and impairments with their daily life activities. It features an advanced and cost-effective infrastructure with Object Vision, Speech Recognition, Autonomous Navigation with Obstacle Avoidance, and a 6 Degree of Freedom Robotic Arm, making it a sophisticated service robot. The robot can differentiate among objects based on class and can anticipate qualities like Gender, Age, and Posture. It can receive commands from a person and perform tasks autonomously, either through manual or autonomous navigation, choosing the shortest path while avoiding dynamic obstacles and navigating through extended waypoints. CHETAK uses the robotic arm with end-effector and vision information to pick and place objects. The whole system is implemented using Robot Operating System(ROS) on Ubuntu platform to integrate individual nodes for complex operations. CHETAK can perform Human-Robot interaction, Object Manipulation, and Gesture recognition, helping the disabled and physically handicapped people by serving drinks, fruits, etc., and visually impaired people by finding objects or people and serving them.<br><a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://amrita.edu/project/service-robot" target="_blank">More Info </a> / <a href="https://www.youtube.com/watch?v=1iNSb_AOZIM" target="_blank"> Demo Video - 1</a> / <a href="https://www.youtube.com/watch?v=L7wPotpw4lE" target="_blank" id="publications"> Demo Video - 2</a></div> </div> </div>

            </div>
        </div>

        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 2em">Publications</h1>
                <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/system_arch.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://link.springer.com/article/10.1007/s41315-023-00272-4" target="_blank">2D-3D hybrid mapping for path planning in autonomous robots</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>,<a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>,  <br><span style="font-style: italic;">International Journal of Intelligent Robotics and Applications</span>, 2020 <br><a href="https://drive.google.com/file/d/1yw3w5XYo1P_WNr17WLIt4v1qVvOI0_dc/view?usp=sharing" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#1" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="1"><div class="card card-body"><pre><code>@InProceedings{author = {Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Rahul Puram}, 
            title = {Multimode Control and Simulation of 6-DOF Robotic Arm in ROS}, 
            booktitle = {Advances in Science, Technology and Engineering Systems Journal (ASTESJ)}, 
            year = {2020}, 
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p2ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://astesj.com/v05/i02/p85/" target="_blank">ROS Based Multimode Control of Wheeled Robot</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>, <a href="https://www.linkedin.com/in/nagasai-chowdary/" target="_blank">NagaSai Thokala</a>, <a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>, <a href="https://www.linkedin.com/in/snaveen37/" target="_blank">Samudrala Naveen</a>  <br><span style="font-style: italic;">Advances in Science, Technology and Engineering Systems Journal (ASTESJ)</span>, 2020 <br><a href="https://drive.google.com/file/d/19DK3Tah7GFJEDjP5nSMqoTnaB_ZEV84S/view?usp=sharing" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#2" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="2"><div class="card card-body"><pre><code>@INPROCEEDINGS{9183013,author={Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Nasa Sai Thokala, Rahul Puram, Samudrala Naveen},
              booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
              title={Implementation of low-cost mobile robot for rescue challenges}, 
              year={2020},
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p3ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://ieeexplore.ieee.org/document/8979113" target="_blank">Robot Operating System Integrated robot control through Secure Shell(SSH)</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>, <a href="https://www.linkedin.com/in/nagasai-chowdary/" target="_blank">NagaSai Thokala</a>, <a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>, <a href="https://www.linkedin.com/in/snaveen37/" target="_blank">Samudrala Naveen</a> <br><span style="font-style: italic;">Institute of Electrical and Electronics Engineers</span>, 2020 <br><a href="https://drive.google.com/file/d/1mAZFORDuh-9nKxadf6Td9gJhYn4jiMa_/view?usp=sharing" target="_blank">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#3" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="3"><div class="card card-body"><pre><code>@INPROCEEDINGS{9171065, author={Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Nasa Sai Thokala, Rahul Puram, Samudrala Naveen},
              booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
              title={PID based locomotion of multi-terrain robot using ROS platform}, 
              year={2020},
            }</pre></code></div></div> </div> </div> </div>

            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p4ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://ieeexplore.ieee.org/document/8777597" target="_blank">Multimode Control of Wheeled Bot Using Python and Virtual Network Computing</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>, <a href="https://www.linkedin.com/in/nagasai-chowdary/" target="_blank">NagaSai Thokala</a>,<a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>, <a href="https://www.linkedin.com/in/snaveen37/" target="_blank">Samudrala Naveen</a>, <br><span style="font-style: italic;">Institute of Electrical and Electronics Engineers</span>, 2018 <br><a href="https://drive.google.com/file/d/1cRg0tcO3cHSInRmH8E6jXEugI1Zg8hfa/view?usp=sharing" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  author={Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Nasa Sai Thokala, Rahul Puram, Samudrala Naveen},
              booktitle={2018 4th International Conference on Computing Communication and Automation (ICCCA)}, 
              title={Keyboard-based control and simulation of 6-DOF robotic arm using ROS}, 
              year={2017},
            }</pre></code></div></div> </div> </div> </div>

            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p5ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://link.springer.com/chapter/10.1007/978-981-16-2164-2_38" target="_blank">Inverse Kinematics of Robot Manipulator Integrated with Image processing Algorithms</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>,  <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>, <a href="https://www.linkedin.com/in/nagasai-chowdary/" target="_blank">NagaSai Thokala</a>,<a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>, <a href="https://www.linkedin.com/in/snaveen37/" target="_blank">Samudrala Naveen</a>, <br><span style="font-style: italic;">Springer</span>, 2018 <br><a href="https://drive.google.com/file/d/1fPx_xMUXV0ZWywBngoe8U1Zv6I_yfW4G/view?usp=sharing" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  {Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Nasa Sai Thokala, Rahul Puram, Samudrala Naveen},
              booktitle={2017 International Conference on Inventive Computing and Informatics (ICICI)}, 
              title={Swarm based autonomous landmine detecting robots}, 
              year={2017},
            }</pre></code></div></div> </div> </div> </div>

            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p5ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://ieeexplore.ieee.org/document/9498567" target="_blank">Robot Operating System Integrated Sensing System and Forward Kinematics of a robot manipulator of a Rescue Robot</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>,  <a href="https://www.linkedin.com/in/santosh-tantravahi-401978188/" target="_blank">Santosh Tantravahi</a>, <a href="https://hsurya08.github.io/" target="_blank">Hemanth Surya</a>, <a href="https://www.linkedin.com/in/nagasai-chowdary/" target="_blank">NagaSai Thokala</a>,<a href="https://www.linkedin.com/in/rahulpuram/" target="_blank">Rahul Puram</a>, <a href="https://www.linkedin.com/in/snaveen37/" target="_blank">Samudrala Naveen</a>, <br><span style="font-style: italic;">Springer</span>, 2018 <br><a href="https://drive.google.com/file/d/1fPx_xMUXV0ZWywBngoe8U1Zv6I_yfW4G/view?usp=sharing" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  {Rajesh Kannan Megalingam, Santosh Tantravahi, Hemanth Surya, Nasa Sai Thokala, Rahul Puram, Samudrala Naveen},
              booktitle={2017 International Conference on Inventive Computing and Informatics (ICICI)}, 
              title={Swarm based autonomous landmine detecting robots}, 
              year={2017},
            }</pre></code></div></div> </div> </div> </div>
            </div>
        </div>




        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em">Competitions</h1>
                      <ul class="competitions__list">

                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/achievements/" target="_blank"><h3 class="competitions__title">Indy Autonomous Challenge (IAC)</h3></a>
                          <p class="competitions__description"><a href="https://www.indyautonomouschallenge.com/">Indy Autonomous Challenge</a> is a broadly collaborative effort that brings together public, private and academic institutions to challenge university students around the world to imagine, invent and prove a new generation of automated vehicle software. </p>
                          <p class="competitions__result_red">Our team Na-Sarathy of HuT Labs, ECE, got 12th position in the recently concluded Hackathon conducted Indy Autonomous Car challenge. In total 30 teams all over the world from top universities participated in this Hackathon.</p>
                        </li>

                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/achievements/" target="_blank"><h3 class="competitions__title">World Robot Summit - Japan 2020</h3></a>
                          <p class="competitions__description">The World Robot Summit (WRS) is Challenge and Expo of its kind to bring together Robot Excellence from around the world. Out Of 256 professional teams that applied from all over the world, We are the only team from India to qualify for the finals and took part in the "Standard disaster challenge". </p>
                          <p class="competitions__result_red">Won 1.25 million JPY and qualified for the finals </p>
                        </li>


                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/achievements/" target="_blank"><h3 class="competitions__title">RoboCup@Home - Germany 2019</h3></a>
                          <p class="competitions__description">The RoboCup@Home league brings together international researchers working on innovative smart robotics technologies to assist emergency responders operating in complex, hazardous environments. It inspires innovations addressing the needs of responders in a wide spectrum of mission requirements involving mobility, sensory perception, planning, mapping, manipulation, assistive behaviors and operator interfaces and their integration in a holistic manner.</p>
                          <p class="competitions__result_red" id="awards">Scured 7th position out of 12 teams from all over the world</p>
                        </li>
                      </ul>
            </div>
        </div>










        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em; margin-top: 2em;">Awards</h1>
                      <ul class="competitions__list">

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Outstanding Student Researcher Award 2019 - Hutlab, Dept of ECE, Amrita University.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Outstanding Student Reseacher Award 2019 - HuT Labs.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Certificate of Excellence 2019 - 2020 by IEEE Student Branch.</h4>
                        </li>
                      </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em; margin-top: 2em;">Talks</h1>
                      <ul class="competitions__list">

                        <li class="competitions__item">
                          <a href="https://www.youtube.com/watch?v=Hvo4u5PTNeg&t=1833s" target="_blank"><h3 class="competitions__title">Multiple Object tracker Webinar</h3></a>
                          <h4 class="competitions__title_award">As a part Multicoreware Inc, ADAS domain employee, I have taken a webinar on Multiple Object Tracker. The webinar contains the concepts of entire pipeline of Multiple Object Tracker which uses Extended Kalman Filter, JPDA, and DBSCAN. </h4>
                        </li>

                        <li class="competitions__item">
                          <a href="" target="_blank"><h3 class="competitions__title">ROS Workshop</h3></a>
                          <h4 class="competitions__title_award">As a part IEEE Student board, I have taken up a ROS workshop in Panchadasha International Robotic tech fest.</h4>
                        </li>

                      </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em; margin-bottom: 1em;">
            
            <div class="col-sm-12" style="">
                <h4 class="end">Website template provided by <a href=https://m-niemeyer.github.io/ target="_blank">Michael Niemeyer</a>. Check out his <a href=https://github.com/m-niemeyer/m-niemeyer.github.io target="_blank">github repository </a> for instructions on how to use it! <h4>
            </div>
    
        </div>
    </div>
			
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
			
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-KKH6VE3K25"></script>
    <script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'G-KKH6VE3K25');
    </script>
</body>

</html>
    
